# -*- coding: utf-8 -*-
"""OverFitOnOneImageBox.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N9Bs5iy5fj5PG0eR76QRm-JTSlHmU_1g
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

cd drive/project_data/final_pascal_npy_datas

import keras
from keras.models import Sequential,load_model
from keras import layers, Model,Input
from keras.layers.normalization import BatchNormalization
from keras import optimizers
import numpy as np
import tensorflow as tf
from keras.optimizers import Adam
from keras import backend as K
from keras.layers.merge import concatenate
from keras.layers.advanced_activations import LeakyReLU
from keras.preprocessing import image
from keras.engine.topology import Layer

"""# Losses"""

def custom_loss(y_true,y_pred):
    loss = tf.reduce_mean(y_true[0,:,0:,0:1]*tf.squared_difference(y_true[0,:,0:,1:5],y_pred[0,:,0:,1:5]))
    conf_loss = tf.reduce_mean(tf.squared_difference(y_true[0,:,0:,0:1],y_pred[0,:,0:,0:1]))
    return loss+conf_loss
# def custom_acc(y_true,y_pred):
#     loss = y_true[0,:,0:,0:1]*tf.squared_difference(y_true[0,:,0:,1:5],y_pred[0,:,0:,1:5])
#     conf_loss = tf.squared_difference(y_true[0,:,0:,0:1],y_pred[0,:,0:,0:1])
    
    
#

def c_loss_one(y_true,y_pred):
  loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
  return loss

X = np.load('X/2007_test.npy')
Y = np.load('Y/2007_test_labels.npy')
print(X.shape)
print(Y.shape)

"""# Custom Layer"""

class NLayer(Layer):

    def __init__(self, output_dim, **kwargs):
        self.output_dim = output_dim
        super(NLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=False)
        super(NLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        return (x/255)

    def compute_output_shape(self, input_shape):
        return input_shape

"""# Model"""

img_input = Input(shape=(300, 300, 3))

# VGG-16
x = NLayer((300,300))(img_input)

x = layers.Conv2D(64, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block1_conv1',
                  trainable=False)(x)
x = layers.Conv2D(64, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block1_conv2',
                  trainable=False)(x)
x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

# Block 2
x = layers.Conv2D(128, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block2_conv1',
                  trainable=False)(x)
x = layers.Conv2D(128, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block2_conv2',
                  trainable=False)(x)
x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

# Block 3
x = layers.Conv2D(256, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block3_conv1',
                  trainable=False)(x)
x = layers.Conv2D(256, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block3_conv2',
                  trainable=False)(x)
x = layers.Conv2D(256, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block3_conv3',
                  trainable=False)(x)
x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

# Block 4
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block4_conv1',
                  trainable=False)(x)
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block4_conv2',
                  trainable=False)(x)
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block4_conv3',
                  trainable=False)(x)
x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

# Block 5
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block5_conv1',
                  trainable=False)(x)
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block5_conv2',
                  trainable=False)(x)
x = layers.Conv2D(512, (3, 3),
                  activation='relu',
                  padding='same',
                  name='block5_conv3',
                  trainable=False)(x)
# VGG over

x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

x = layers.Conv2D(100, (5, 5),
                  activation='relu',
                  padding='same',
                  name='block6_conv1',
                  trainable=False)(x)
x = layers.Conv2D(50, (5, 5),
                  activation='relu',
                  padding='same',
                  name='block6_conv2',
                  trainable=False)(x)
x = layers.Conv2D(5, (5, 5),
                  activation='relu',
                  padding='same',
                  name='block6_conv3',
                  trainable=False)(x)

model = Model(img_input, x)

pred = model.predict(X[0:1])
pred.shape



model.load_weights('classifier_weights.h5',by_name=True)

optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss=custom_loss, optimizer="SGD",metrics=['accuracy'])

history0 = model.fit(X, Y, batch_size=25, epochs=100, verbose=1)

out = model.predict(X[0:1])

out.shape

out = np.reshape(out,(81,5))
act = np.reshape(Y[0], (81,5))

print(out.shape)
print(act.shape)

for i in range(81):
  if act[i,0] == 1:
    print("################")
    print("i = ", i)
    print(act[i])

for i in range(81):
  #if out[i,0] == 1:
    print("################")
    print("i = ", i)
    print(act[i])
    
    
"""
ACTUAL VALUES
----------------------------------------------------------------------
i =  32
1.14730878e-03    2.03666667e-03    1.38810198e-03    8.73333333e-04
----------------------------------------------------------------------
0.00169972        0.0017            0.00324835        0.00324
----------------------------------------------------------------------
PREEDECTED VALUES
----------------------------------------------------------------------
at i = 32
1.14730878e-03    2.03666667e-03    1.38810198e-03    8.73333333e-04
----------------------------------------------------------------------
ar i = 40
0.00169972        0.0017            0.00324835        0.00324 
----------------------------------------------------------------------
"""





pred = model.predict(X_try[0:1])

pred.shape













X_try = np.zeros((100,300,300,3))
Y_try = np.zeros((100,9,9,5))
for i in range(5):
  X_try[i] = X[0]/300
  Y_try[i] = Y[0]/300
  
for b in range(5):
  for i in range(9):
    for j in range(9):
      Y_try[b,i,j,0] *= 300

np.save('WrongX',X_try)
np.save('WrongY',Y_try)

ls

X_try.shape

Y_try.shape

Y_try[1,3]



"""# C ustom LAyer"""

import keras
from keras.models import Sequential,load_model
from keras import layers, Model,Input
from keras.layers.normalization import BatchNormalization
from keras import optimizers
import numpy as np
import tensorflow as tf
from keras.optimizers import Adam
from keras import backend as K
from keras.layers.merge import concatenate
from keras.layers.advanced_activations import LeakyReLU
from keras.preprocessing import image
from keras.engine.topology import Layer

from keras import backend as K
from keras.engine.topology import Layer
import numpy as np

class MyLayer(Layer):

    def __init__(self, output_dim, **kwargs):
        self.output_dim = output_dim
        super(MyLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)
        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        return K.dot(x, self.kernel)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)

img_input = Input(shape=(300, 300, 3))

x = MyLayer((300,300,3))(img_input)

