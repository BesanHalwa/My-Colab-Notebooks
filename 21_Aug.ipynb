{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21 Aug.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "CalxfF8g9xrU",
        "FzM-qbgi9-kS",
        "dRbGIwxF-D8P",
        "Xt4jdO2M_nHO",
        "Y617ohldEZow",
        "Nr8TPTZt-moL",
        "4jtp71uSBMWx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalxfF8g9xrU",
        "colab_type": "text"
      },
      "source": [
        "# Collab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZvCwUpq9pV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0rmav7S968B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzM-qbgi9-kS",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVItZUJw96-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvRbGEaP97A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRbGIwxF-D8P",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC9hqRrG97DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import tensorflow\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU, Softmax\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from __future__ import division\n",
        "\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt4jdO2M_nHO",
        "colab_type": "text"
      },
      "source": [
        "# Model Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tftJc8so_qjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_height = 300 # Height of the model input images\n",
        "img_width = 300 # Width of the model input images\n",
        "img_channels = 3 # Number of color channels of the model input images\n",
        "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
        "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
        "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
        "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
        "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
        "scales = scales_pascal\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
        "two_boxes_for_ar1 = True\n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y617ohldEZow",
        "colab_type": "text"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUAuuJ3n97NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSDLoss:\n",
        "\n",
        "    def __init__(self,\n",
        "                 neg_pos_ratio=3,\n",
        "                 n_neg_min=0,\n",
        "                 alpha=1.0):\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.n_neg_min = n_neg_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def smooth_L1_loss(self, y_true, y_pred):\n",
        "\n",
        "        absolute_loss = tf.abs(y_true - y_pred)\n",
        "        square_loss = 0.5 * (y_true - y_pred)**2\n",
        "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
        "        return tf.reduce_sum(l1_loss, axis=-1)\n",
        "\n",
        "    def log_loss(self, y_true, y_pred):\n",
        "        \n",
        "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
        "        y_pred = tf.maximum(y_pred, 1e-15)\n",
        "        # Compute the log loss\n",
        "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
        "        return log_loss\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
        "        self.n_neg_min = tf.constant(self.n_neg_min)\n",
        "        self.alpha = tf.constant(self.alpha)\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell.\n",
        "\n",
        "        # 1: Compute the losses for class and box predictions for every box.\n",
        "\n",
        "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)\n",
        "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)\n",
        "\n",
        "        # 2: Compute the classification losses for the positive and negative targets.\n",
        "\n",
        "        # Create masks for the positive and negative ground truth classes.\n",
        "        negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
        "        positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
        "\n",
        "        # Count the number of positive boxes (classes 1 to n) in y_true across the whole batch.\n",
        "        n_positive = tf.reduce_sum(positives)\n",
        "\n",
        "        # Now mask all negative boxes and sum up the losses for the positive boxes PER batch item\n",
        "        # (Keras loss functions must output one scalar loss value PER batch item, rather than just\n",
        "        # one scalar for the entire batch, that's why we're not summing across all axes).\n",
        "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "        # Compute the classification loss for the negative default boxes (if there are any).\n",
        "\n",
        "        # First, compute the classification loss for all negative boxes.\n",
        "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
        "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
        "\n",
        "        # Compute the number of negative examples we want to account for in the loss.\n",
        "        # We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`, but at least `self.n_neg_min` (unless `n_neg_loses` is smaller).\n",
        "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
        "\n",
        "        # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
        "        # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
        "        def f1():\n",
        "            return tf.zeros([batch_size])\n",
        "        # Otherwise compute the negative loss.\n",
        "        def f2():\n",
        "            # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
        "            # belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model\n",
        "            # predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.\n",
        "\n",
        "            # To do this, we reshape `neg_class_loss_all` to 1D...\n",
        "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
        "            # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
        "            values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
        "                                          k=n_negative_keep,\n",
        "                                          sorted=False) # We don't need them sorted.\n",
        "            # ...and with these indices we'll create a mask...\n",
        "            negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
        "                                           updates=tf.ones_like(indices, dtype=tf.int32),\n",
        "                                           shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
        "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
        "            # ...and use it to keep only those boxes and mask all other classification losses\n",
        "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
        "            return neg_class_loss\n",
        "\n",
        "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
        "\n",
        "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
        "\n",
        "        # 3: Compute the localization loss for the positive targets.\n",
        "        #    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).\n",
        "\n",
        "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "        # 4: Compute the total loss.\n",
        "\n",
        "        total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
        "        # Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case\n",
        "        # because the relevant criterion to average our loss over is the number of positive boxes in the batch\n",
        "        # (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging\n",
        "        # over the batch size, we'll have to multiply by it.\n",
        "        total_loss = total_loss * tf.to_float(batch_size)\n",
        "\n",
        "        return total_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQS9FnfK97Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_loss(y_pred, y_pred):\n",
        "  return tf.reduce_sum(tf.abs(tf.subtract(y_pred, y_pred)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "760-pg9397R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr8TPTZt-moL",
        "colab_type": "text"
      },
      "source": [
        "# Additional Layer defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaETiJ3797Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class L2Normalization(Layer):\n",
        "    '''\n",
        "    Performs L2 normalization on the input tensor with a learnable scaling parameter\n",
        "    as described in the paper \"Parsenet: Looking Wider to See Better\" (see references)\n",
        "    and as used in the original SSD model.\n",
        "\n",
        "    Arguments:\n",
        "        gamma_init (int): The initial scaling parameter. Defaults to 20 following the\n",
        "            SSD paper.\n",
        "\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "\n",
        "    Returns:\n",
        "        The scaled tensor. Same shape as the input tensor.\n",
        "\n",
        "    References:\n",
        "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
        "    '''\n",
        "\n",
        "    def __init__(self, gamma_init=20, **kwargs):\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            self.axis = 3\n",
        "        else:\n",
        "            self.axis = 1\n",
        "        self.gamma_init = gamma_init\n",
        "        super(L2Normalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
        "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
        "        self.trainable_weights = [self.gamma]\n",
        "        super(L2Normalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output = K.l2_normalize(x, self.axis)\n",
        "        return output * self.gamma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'gamma_init': self.gamma_init\n",
        "        }\n",
        "        base_config = super(L2Normalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZbcQQxr97F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AnchorBoxes(Layer):\n",
        "\n",
        "  \n",
        "  \n",
        "    def __init__(self,\n",
        "                 img_height, img_width, this_scale, next_scale,\n",
        "                 aspect_ratios=[0.5, 1.0, 2.0], two_boxes_for_ar1=True,\n",
        "                 this_steps=None, this_offsets=None, clip_boxes=False,\n",
        "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                 coords='centroids', normalize_coords=False, **kwargs):\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.this_scale = this_scale\n",
        "        self.next_scale = next_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        self.this_steps = this_steps\n",
        "        self.this_offsets = this_offsets\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.variances = variances\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        # Compute the number of boxes per cell\n",
        "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
        "            self.n_boxes = len(aspect_ratios) + 1\n",
        "        else:\n",
        "            self.n_boxes = len(aspect_ratios)\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        '''\n",
        "        Return an anchor box tensor based on the shape of the input tensor.\n",
        "\n",
        "        Note that this tensor does not participate in any graph computations at runtime. It is being created\n",
        "        as a constant once during graph creation and is just being output along with the rest of the model output\n",
        "        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n",
        "        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n",
        "\n",
        "        Arguments:\n",
        "            x (tensor): 4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n",
        "                layer must be the output of the localization predictor layer.\n",
        "        '''\n",
        "\n",
        "        # Compute box width and height for each aspect ratio\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in self.aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = self.this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_height = self.this_scale * size / np.sqrt(ar)\n",
        "                box_width = self.this_scale * size * np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "\n",
        "        # We need the shape of the input tensor\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = x._keras_shape\n",
        "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = x._keras_shape\n",
        "\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (self.this_steps is None):\n",
        "            step_height = self.img_height / feature_map_height\n",
        "            step_width = self.img_width / feature_map_width\n",
        "        else:\n",
        "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
        "                step_height = self.this_steps[0]\n",
        "                step_width = self.this_steps[1]\n",
        "            elif isinstance(self.this_steps, (int, float)):\n",
        "                step_height = self.this_steps\n",
        "                step_width = self.this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (self.this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
        "                offset_height = self.this_offsets[0]\n",
        "                offset_width = self.this_offsets[1]\n",
        "            elif isinstance(self.this_offsets, (int, float)):\n",
        "                offset_height = self.this_offsets\n",
        "                offset_width = self.this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
        "\n",
        "\n",
        "        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.clip_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "  \n",
        "        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
        "        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
        "\n",
        "        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
        "        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
        "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
        "\n",
        "        return boxes_tensor\n",
        "      \n",
        "      \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "        batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jtp71uSBMWx",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWAzF3RZ97K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ssd_300(l2_regularization=0.0005,\n",
        "            min_scale=None,\n",
        "            max_scale=None,\n",
        "            scales=None,\n",
        "            aspect_ratios_global=None,\n",
        "            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5]],\n",
        "            two_boxes_for_ar1=True,\n",
        "            steps=[8, 16, 32, 64, 100, 300],\n",
        "            offsets=None,\n",
        "            clip_boxes=False,\n",
        "            variances=[0.1, 0.1, 0.2, 0.2],\n",
        "            coords='centroids',\n",
        "            normalize_coords=True,\n",
        "            subtract_mean=[123, 117, 104],\n",
        "            divide_by_stddev=None,\n",
        "            swap_channels=[2, 1, 0],\n",
        "            confidence_thresh=0.01,\n",
        "            iou_threshold=0.45,\n",
        "            top_k=200,\n",
        "            nms_max_output_size=400,\n",
        "            return_predictor_sizes=False):\n",
        "\n",
        "    n_classes = 20\n",
        "    n_classes += 1 # Account for the background class.\n",
        "    n_predictor_layers = 6 # The number of predictor conv layers in the network is 6 for the original SSD300.\n",
        "    l2_reg = l2_regularization # Make the internal name shorter.\n",
        "    img_height, img_width, img_channels = 300, 300, 3\n",
        "\n",
        "   ############################################################################\n",
        "    # Compute the anchor box parameters.\n",
        "    ############################################################################\n",
        "\n",
        "    # Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.\n",
        "    if aspect_ratios_per_layer:\n",
        "        aspect_ratios = aspect_ratios_per_layer\n",
        "    else:\n",
        "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
        "\n",
        "    # Compute the number of boxes to be predicted per cell for each predictor layer.\n",
        "    # We need this so that we know how many channels the predictor layers need to have.\n",
        "    if aspect_ratios_per_layer:\n",
        "        n_boxes = []\n",
        "        for ar in aspect_ratios_per_layer:\n",
        "            if (1 in ar) & two_boxes_for_ar1:\n",
        "                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n",
        "            else:\n",
        "                n_boxes.append(len(ar))\n",
        "    else: # If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer\n",
        "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "            n_boxes = len(aspect_ratios_global) + 1\n",
        "        else:\n",
        "            n_boxes = len(aspect_ratios_global)\n",
        "        n_boxes = [n_boxes] * n_predictor_layers\n",
        "\n",
        "    if steps is None:\n",
        "        steps = [None] * n_predictor_layers\n",
        "    if offsets is None:\n",
        "        offsets = [None] * n_predictor_layers\n",
        "\n",
        "    ############################################################################\n",
        "    # Define functions for the Lambda layers below.\n",
        "    ############################################################################\n",
        "\n",
        "    def identity_layer(tensor):\n",
        "        return tensor\n",
        "\n",
        "    def input_mean_normalization(tensor):\n",
        "        return tensor - np.array(subtract_mean)\n",
        "\n",
        "    def input_stddev_normalization(tensor):\n",
        "        return tensor / np.array(divide_by_stddev)\n",
        "\n",
        "    def input_channel_swap(tensor):\n",
        "        if len(swap_channels) == 3:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]]], axis=-1)\n",
        "        elif len(swap_channels) == 4:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]], tensor[...,swap_channels[3]]], axis=-1)\n",
        "\n",
        "    ############################################################################\n",
        "    # Build the network.\n",
        "    ############################################################################\n",
        "    img_height, img_width, img_channels = 300, 300, 3\n",
        "    x = Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "    # The following identity layer is only needed so that the subsequent lambda layers can be optional.\n",
        "    x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
        "    if not (subtract_mean is None):\n",
        "        x1 = Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n",
        "    if not (divide_by_stddev is None):\n",
        "        x1 = Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n",
        "  \n",
        "  \n",
        "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block1_conv1', trainable=False)(x1)\n",
        "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block1_conv2', trainable=False)(conv1_1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n",
        "\n",
        "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block2_conv1', trainable=False)(pool1)\n",
        "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block2_conv2', trainable=False)(conv2_1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n",
        "\n",
        "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block3_conv1', trainable=False)(pool2)\n",
        "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block3_conv2', trainable=False)(conv3_1)\n",
        "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block3_conv3', trainable=False)(conv3_2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n",
        "\n",
        "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block4_conv1', trainable=False)(pool3)\n",
        "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block4_conv2', trainable=False)(conv4_1)\n",
        "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block4_conv3', trainable=False)(conv4_2)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n",
        "\n",
        "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block5_conv1', trainable=False)(pool4)\n",
        "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block5_conv2', trainable=False)(conv5_1)\n",
        "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='block5_conv3', trainable=False)(conv5_2)\n",
        "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n",
        "\n",
        "    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
        "\n",
        "    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
        "\n",
        "    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
        "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
        "    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
        "\n",
        "    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
        "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
        "    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
        "\n",
        "\n",
        "    # Feed conv4_3 into the L2 normalization layer\n",
        "    conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
        "\n",
        "    ### Build the convolutional predictor layers on top of the base network\n",
        "\n",
        "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
        "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
        "    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
        "    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
        "    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
        "    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
        "    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
        "    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n",
        "    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
        "    fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
        "    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
        "    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
        "\n",
        "    ### Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\n",
        "\n",
        "    # Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n",
        "                                             two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0], clip_boxes=clip_boxes,\n",
        "                                             variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1], clip_boxes=clip_boxes,\n",
        "                                    variances=variances, coords=coords, normalize_coords=normalize_coords, name='fc7_mbox_priorbox')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n",
        "\n",
        "    ### Reshape\n",
        "\n",
        "    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n",
        "    # We want the classes isolated in the last axis to perform softmax on them\n",
        "    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
        "    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
        "    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
        "    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
        "    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n",
        "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
        "    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_loc_reshape = Reshape((-1, 4), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_loc_reshape = Reshape((-1, 4), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_loc_reshape = Reshape((-1, 4), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n",
        "    # Reshape the ancho\\r box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox_reshape = Reshape((-1, 8), name='conv4_3_norm_mbox_priorbox_reshape')(conv4_3_norm_mbox_priorbox)\n",
        "    fc7_mbox_priorbox_reshape = Reshape((-1, 8), name='fc7_mbox_priorbox_reshape')(fc7_mbox_priorbox)\n",
        "    conv6_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv6_2_mbox_priorbox_reshape')(conv6_2_mbox_priorbox)\n",
        "    conv7_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv7_2_mbox_priorbox_reshape')(conv7_2_mbox_priorbox)\n",
        "\n",
        "    ### Concatenate the predictions from the different layers\n",
        "\n",
        "    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
        "    # so we want to concatenate along axis 1, the number of boxes per layer\n",
        "    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
        "    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
        "                                                       fc7_mbox_conf_reshape,\n",
        "                                                       conv6_2_mbox_conf_reshape,\n",
        "                                                       conv7_2_mbox_conf_reshape])\n",
        "\n",
        "    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n",
        "    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
        "                                                     fc7_mbox_loc_reshape,\n",
        "                                                     conv6_2_mbox_loc_reshape,\n",
        "                                                     conv7_2_mbox_loc_reshape])\n",
        "\n",
        "    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
        "    mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n",
        "                                                               fc7_mbox_priorbox_reshape,\n",
        "                                                               conv6_2_mbox_priorbox_reshape,\n",
        "                                                               conv7_2_mbox_priorbox_reshape])\n",
        "\n",
        "    # The box coordinate predictions will go into the loss function just the way they are,\n",
        "    # but for the class predictions, we'll apply a softmax activation layer first\n",
        "    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
        "\n",
        "    # Concatenate the class and box predictions and the anchors to one large predictions vector\n",
        "    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
        "    predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n",
        "\n",
        "\n",
        "    model = Model(inputs=x, outputs=predictions)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}