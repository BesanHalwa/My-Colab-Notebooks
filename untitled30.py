# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15vdltzG9cQIEW2Y8qpBhhdsOHizUwxBX
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

ls

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Reshape,Activation
from keras import backend as k
import matplotlib.pyplot as plt
import numpy as np
from keras.layers.normalization import BatchNormalization

x_ = np.load('coco_npy/batch0.npy')
y_ = np.load('coco_npy/Y_coco.npy')



x_ = np.load('coco_npy/batch1.npy')

x_ = np.load('coco_npy/batch2.npy')



from keras import optimizers
from keras import losses

input_shape = (300,300,3)
num_category = 91

model = Sequential()

model.add(Conv2D(32,(3, 3),activation='relu',input_shape=input_shape))

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (4, 4), activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(90, (4, 4), activation='relu'))


model.add(Conv2D(92, (5, 5), activation='relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(92, (5, 5), activation='relu'))

model.add(Conv2D(180, (6, 6), activation='relu'))

model.add(Conv2D(180, (6, 6), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Conv2D(10, (4, 4), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Conv2D(10, (3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Conv2D(10, (3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())


model.add(BatchNormalization())
model.add(Dense(num_category,activation='softmax'))

adm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)



model.compile(loss=losses.categorical_crossentropy, 
              optimizer=adm,
              metrics=['accuracy'])

batch_size = 10
num_epoch = 1
#model training
model_log = model.fit(x_,y_ ,
          batch_size=batch_size,
          epochs=num_epoch,
          verbose=1)

import os
# plotting the metrics
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(model_log.history['acc'])
#plt.plot(model_log.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(model_log.history['loss'])
#plt.plot(model_log.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

fig

batch_size = 10
num_epoch = 1
#model training
model_log = model.fit(x_,y_ ,
          batch_size=batch_size,
          epochs=num_epoch,
          verbose=1)

import os
import m
# plotting the metrics
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(model_log.history['acc'])
plt.plot(model_log.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(model_log.history['loss'])
plt.plot(model_log.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

fig

batch_size = 10
num_epoch = 1
#model training
model_log = model.fit(x_,y_ ,
          batch_size=batch_size,
          epochs=num_epoch,
          verbose=1)

import os
import m
# plotting the metrics
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(model_log.history['acc'])
plt.plot(model_log.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(model_log.history['loss'])
plt.plot(model_log.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

fig

batch_size = 50
num_epoch = 1
#model training
model_log = model.fit(x_,y_ ,
          batch_size=batch_size,
          epochs=num_epoch,
          verbose=1)

